产品侧（终端用户视角）

| 产品入口 | 功能 | 模型系列 | 模态 | 版本 | 程序入口 | 组织 | 推理类型 | 规模/精度 | 备注 |
|---|---|---|---|---|---|---|---|---|---|
| DeepSeek 聊天官网 | 日常对话 | DeepSeek-V3 | 纯文本 | 2024-12 | Web、微信小程序、手机 App | 深度求索 | 非推理（快） | 671B-MoE / INT8 | 通用场景默认模型 |
| DeepSeek 聊天官网 | 深度思考 | DeepSeek-R1 | 纯文本 | 2025-01 | Web、微信小程序、手机 App | 深度求索 | 推理（Long-CoT） | 671B-MoE / INT8 | 开「深度思考」即走 R1 |
| DeepSeek 绘图页 | 文生图 | Janus-Pro-7B | 文本→图像 | 2025-01 | Web、PC 客户端 | 深度求索 | 非推理（扩散） | 7B / FP16 | 独立入口，暂不支持 API |
| 小程序「深小问」 | 语音对话 | DeepSeek-V3-TTS | 文本↔语音 | 2025-02 | 微信小程序 | 深度求索 | 非推理 | 1.2B / INT8 | 免费额度 200 次/日 |

开发侧（API / 本地部署视角）

| base_id | model_name | 能力关键词 | 上下文长度 | 最大输出 | 是否含思维链 | 单价 (input/output) | 本地可部署 | 许可证 | 备注 |
|---|---|---|---|---|---|---|---|---|---|
| v3 | deepseek-chat | 通用对话、代码、工具调用 | 128 k | 4 k | 否 | 0.0003 $/0.0003 $/1k | ✅ 7B-70B 蒸馏 | MIT | 默认 API 模型 |
| r1 | deepseek-reasoner | 数学、逻辑、代码推理 | 128 k | 8 k | 是（流式 reasoning_content） | 0.0009 $/0.0009 $/1k | ✅ 1.5B-70B 蒸馏 | MIT | 同权重长链模式 |
| janus-pro-7b | janus-pro-7b | 文生图 & 图理解 | 8 k | 2 k | 否 | 暂未公开 | ✅ 7B | MIT | 视觉编码器 448×448 |
| embedding | deepseek-embedding | 向量化 | 2 k | 2048 dim | — | 0.0001 $/1k | ✅ 1.5B | MIT | 维度 2048，余弦归一化 |
| tts-1 | deepseek-tts | 文本→语音 | 4 k | 音频 10 min | — | 0.015 $/1k 字符 | ✅ 1.2B | MIT | 仅中文，实时率 < 0.1 |

说明
1. base_id：对应官方  /v1/models  返回的  id  前缀，用于脚本批量调用。
2. model_name：实际传给  model="xxx"  的字符串，大小写敏感。
3. 单价为 2025-06 官方美元报价，MTok=1000 tokens；TTS 按字符计费。
4. 「本地可部署」栏：✅ 表示已公开 HuggingFace 权重，可离线运行；尺寸栏给出常见蒸馏规模。
5. 全部模型遵循 MIT 协议，可商用、可二次分发，只需保留版权文件。

和kimi的差异，可以一句话总结为：
 - DeepSeek：V3 与 R1 是两条独立基座权重，产品里用「深度思考」开关做模型级路由；开发者调 API 时必须显式换 model_name 才能切换。
 - Kimi：K2 与 K2-Thinking 是同一基座的两份训练权重，产品里用「深度思考」开关只是打开长链推理模式；开发者仍传同一  base_id="k2" ，仅改  model="kimi-k2-thinking"  即可，无需换基座。
 
因此：
- DeepSeek 的「深度思考」= 换模型；
- Kimi 的「深度思考」= 同一模型换推理策略。

### 豆包的也暂时先放这里

产品侧

| 产品入口 | 功能 | 模型系列 | 模态 | 版本 | 程序入口 | 组织 | 推理类型 | 规模/精度 | 备注 |
|---|---|---|---|---|---|---|---|---|---|
| 豆包 App / 网页 | 日常聊天 | 豆包·通用模型 | 纯文本 | 3.0 | App、Web、小程序、PC 客户端 | 字节跳动 | 非推理 | 未公开 | 默认模式，低延迟 |
| 豆包 App / 网页 | 深度思考 | 豆包·思考模型 | 纯文本 | 3.0 | App、Web、PC 客户端 | 字节跳动 | 推理（Long-CoT） | 未公开 | 设置内开启「深度思考」 |
| 豆包 App·智能体 | 文生图 | 豆包·文生图模型 | 文本→图像 | 3.0 | App、Web | 字节跳动 | 非推理（扩散） | 未公开 | 独立入口，暂不支持 API |
| 豆包 App·智能体 | 语音通话 | 豆包·语音模型 | 文本↔语音 | 3.0 | App | 字节跳动 | 非推理 | 未公开 | 实时双向通话 |


开发侧：豆包（火山引擎方舟平台）

| base_id | model_name | 能力关键词 | 上下文长度 | 最大输出 | 是否含思维链 | 单价 (input/output) | 本地可部署 | 许可证 | 备注 |
|---|---|---|---|---|---|---|---|---|---|
| doubao-lite-128k | doubao-lite-128k | 通用对话、总结 | 128 k | 4 k | 否 | 0.0008 元/0.0008 元/1k | ❌ | 火山商用 | 速度最快，成本低 |
| doubao-pro-128k | doubao-pro-128k | 复杂问答、代码 | 128 k | 4 k | 否 | 0.001 元/0.001 元/1k | ❌ | 火山商用 | 精度更高 |
| doubao-thinking-128k | doubao-thinking-128k | 数学、逻辑推理 | 128 k | 8 k | 是（流式 reasoning） | 0.0015 元/0.003 元/1k | ❌ | 火山商用 | 开启「深度思考」时调用 |
| doubao-vision-32k | doubao-vision-32k | 图→文、OCR | 32 k | 2 k | 否 | 0.0012 元/0.0012 元/1k | ❌ | 火山商用 | 视觉理解 |
| doubao-embedding | doubao-embedding | 向量化 | 2 k | 2048 dim | — | 0.0002 元/1k | ❌ | 火山商用 | 维度 2048，余弦归一化 |

### 关于embedding

embedding（嵌入）模型做的事一句话就能概括：
把任意文本/图像/音频变成一串固定长度的数字向量（embedding），让计算机可快速计算“语义相似度”——是传统关键词搜索的升级版。

1. 它在开发侧表格里为什么单列一行？
 大模型 API 把「embedding」当成一种独立能力计费：
– 输入：任意字符串
– 输出：float 数组（768/1024/2048 维…）
– 用途：RAG、知识库、推荐、聚类、分类
因此它和 chat / reasoning / vision 并列收费，自然单独占一行。

2. 技术原理（无需数学）
训练目标：让「语义相近」的句子在向量空间里距离短，「语义无关」的距离长。
结果：
 - “苹果发布会” ↔ “iPhone 新品” 距离 ≈ 0.12
 - “苹果发布会” ↔ “红富士水果” 距离 ≈ 0.78
靠这个距离即可做语义检索，而不再依赖关键词完全匹配。

3. 业界常见 embedding 模型（都有对应 API）
- OpenAI：text-embedding-3-small / large（1536–3072 维）
- Google：gecko-001 / gemini-embedding（768 维）
- Anthropic：无独立 embed，推荐用 Cohere 或自家 rerank
 国内
- 豆包：doubao-embedding（2048 维）
- DeepSeek：deepseek-embedding（1024 维）
- Kimi：kimi-embedding（2048 维）
- 智谱：embedding-v3（1024 维）
- 阿里：text-embedding-v2（768 维）

3. 典型使用流程（以 RAG 为例）
① 文档切片 → ② 调 embedding API 得到向量 → ③ 写入向量库（FAISS/PGVector/Milvus）
用户提问 → ④ 把问题也变成向量 → ⑤ 向量库 Top-K 召回相关段 → ⑥ 把段落塞进大模型 prompt → ⑦ 生成回答
embedding 只做 ②④ 两步，却是 RAG 的底座。

4. 小结一句话
embedding 就是语义翻译器：任何内容 → 向量；
所有主流大模型厂都会提供，否则自家知识库、插件、搜索就玩不转——所以你会在 Kimi/DeepSeek/豆包的开发侧表格里清一色看到它。

5. embedding 本身没有行业强制标准，各家可以决定「是否开放、维度多大、价格多少」；
但接口形式基本都对齐 OpenAI 的  /v1/embeddings  路径与请求/响应字段，所以开发者可以「一键迁移」。

6. embedding 模型通常独立于生成模型（Separate Base），负责把文本→向量；
下游的 RAG/检索/排序 用它拿到向量后，再把召回结果送进大模型 prompt，从而让大模型获得更精准、实时的上下文，而不是靠它自己「理解」向量距离。